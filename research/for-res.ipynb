{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666e1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32654c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "\n",
    "X, y = data.data, data.target\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "sc_train, sc_val = sc.transform(X_train), sc.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b0eae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00473375 -0.04952227 -0.02969809 -0.04704585  0.04477205 -0.00676507\n",
      " -0.0137238  -0.00067061 -0.00737889 -0.00230653 -0.04665532 -0.03092174\n",
      " -0.01614465  0.00844858  0.04100564  0.01961783  0.00930797  0.0084561\n",
      " -0.04328236  0.01617862 -0.00336471 -0.03588656 -0.03810471 -0.01190325\n",
      " -0.01838014  0.0101337   0.02391039  0.00402939 -0.00948756 -0.03420626] [0.0466756]\n"
     ]
    }
   ],
   "source": [
    "w = np.random.uniform(low=-0.05, high=0.05, size=(X.shape[1]))\n",
    "b = np.random.uniform(low=-0.05, high=0.05, size=(1))\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e8d4c",
   "metadata": {},
   "source": [
    "## Gradient descent based method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b987064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w, b):\n",
    "    return np.dot(x, w) + b\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "def grad_descent(x, w, y_true, y_hat):\n",
    "    dw = (1/len(y_true)) * np.dot(x.T, (y_hat - y_true))\n",
    "    db = (1 / len(y_true) *np.sum(y_hat - y_true))\n",
    "    return dw, db\n",
    "\n",
    "def loss_func(y_true, y_hat, epsilon=1e-6):\n",
    "    y_hat = np.clip(y_hat, a_min = epsilon, a_max= 1 - epsilon)\n",
    "    f1 = y_true * np.log(y_hat)\n",
    "    f2 = (1 - y_true ) * np.log( 1 - y_hat + epsilon) \n",
    "    f = f1 + f2\n",
    "    return - np.mean(f)\n",
    "\n",
    "def accuracy(y_true, y_hat):\n",
    "    return np.sum(y_true == y_hat) / len(y_true)\n",
    "\n",
    "def update(w, b, dw, db, lr):\n",
    "    w = w - lr * dw\n",
    "    b = b - lr * db\n",
    "    return w, b\n",
    "\n",
    "def argmaxThis(pred):\n",
    "    argmaxed = []\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] > 0.5:\n",
    "            argmaxed.append(1)\n",
    "        else : \n",
    "            argmaxed.append(0)\n",
    "    return argmaxed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "595aea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist, val_loss_hist = [], []\n",
    "\n",
    "for _ in range(1300):\n",
    "    y_hat = sigmoid(forward(sc_train, w, b))\n",
    "    dw, db = grad_descent(sc_train, w, y_train, y_hat)\n",
    "    w, b = update(w, b, dw, db, 0.01)\n",
    "    loss = loss_func(y_train, y_hat)\n",
    "    val_hat = sigmoid(forward(sc_val, w, b))\n",
    "    val_loss = loss_func(y_val, val_hat)\n",
    "    loss_hist.append(loss)\n",
    "    val_loss_hist.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c6500b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(sc_train, w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77679d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512, 30), (30,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_train.shape, w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c805ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = sigmoid(np.dot(sc_train, w) + b)\n",
    "train_acc = accuracy(y_train, argmaxThis(train_pred))\n",
    "\n",
    "val_pred =  sigmoid(np.dot(sc_val, w) + b)\n",
    "val_acc = accuracy(y_val, argmaxThis(val_pred))\n",
    "\n",
    "print(\"Train Accuracy : \", train_acc)\n",
    "print(\"Validation Accuracy : \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d55942",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"Loss History\")\n",
    "plt.plot(loss_hist, color='b', label='Train')\n",
    "plt.plot(val_loss_hist, color='r', label='Validation')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5281b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression :\n",
    "    def __init__(self, \n",
    "                 steps : int = 300, \n",
    "                 epsilon : float = 1e-6,\n",
    "                 lr : float = 0.01,\n",
    "                 threshold : float = 0.5,\n",
    "                 use_bias : bool = True,\n",
    "                 init : str = \"normal\"):\n",
    "        self.use_bias = use_bias\n",
    "        self.steps = steps\n",
    "        self.epsilon = epsilon\n",
    "        self.init = init.lower()\n",
    "        self.lr = lr\n",
    "        assert threshold < 1.0 and threshold > 0.0, f\"Threshold has to be between 0 and 1 !\"\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit(self, X, y, batch_size=64):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        assert batch_size <= len(X), \"Batch size can't be bigger than size of the data\"\n",
    "        assert len(np.unique(y)) == 2, \"Logistic Regression can only be used as binary classification. Use Softmax Regression instead.\"\n",
    "        N, m = X.shape\n",
    "        if self.init == \"normal\":\n",
    "            w = np.random.normal(loc=0., scale=0.05, size=m)\n",
    "            b = np.random.normal(loc=0., scale=0.05, size=1)\n",
    "        elif self.init == \"uniform\":\n",
    "            w = np.random.uniform(low=-0.05, high=0.05, size=m)\n",
    "            b = np.random.uniform(low=-0.05, high=0.05, size=1)\n",
    "        else :\n",
    "            raise ValueError(\"Weights initializer is not valid. Use uniform or normal.\")\n",
    "        assert len(X) == len(y), f\"Feature size {len(X)} has not the same as label size {len(y)}\"            \n",
    "        losses = []\n",
    "        \n",
    "        for _ in range(self.steps):\n",
    "            perm = np.random.permutation(N)\n",
    "            X_shuf, y_shuf = X[perm], y[perm]           \n",
    "            for batch in range(0, N, batch_size):\n",
    "                X_batch, y_batch = X_shuf[batch:batch+batch_size], y_shuf[batch:batch+batch_size]\n",
    "                if self.use_bias :\n",
    "                    y_prob = self.__sigmoid(np.dot(X_batch, w) + b)\n",
    "                    dw, db = self.__gradientDescent(X_batch, y_batch, y_prob)\n",
    "                    w = w - self.lr * dw\n",
    "                    b = b - self.lr * db\n",
    "                    loss = self.__logloss(y_batch, y_prob, self.epsilon)\n",
    "                    losses.append(loss)\n",
    "                else : \n",
    "                    y_prob = self.__sigmoid(np.dot(X_batch, w)) # feedforward\n",
    "                    dw = self.__gradientDescent(X_batch, y_batch, y_prob)\n",
    "                    w = w - self.lr * dw\n",
    "                    loss = self.__logloss(y_batch, y_prob, self.epsilon)\n",
    "                    losses.append(loss)\n",
    "        self.w, self.b = w, b\n",
    "        self.loss_hist = np.array(losses)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        assert X.shape[1] == len(self.w), \"Different shape with fitted data !\"\n",
    "        if self.use_bias :\n",
    "            z = self.__sigmoid(np.dot(X, self.w) + self.b )\n",
    "        else : \n",
    "            z = self.__sigmoid(np.dot(X, self.w))\n",
    "        return np.array([1 if i > self.threshold else 0 for i in z])\n",
    "    \n",
    "    def __gradientDescent(self, X, y_true, y_hat):\n",
    "        if self.use_bias : \n",
    "            dw = (1 / len(X) * np.dot(X.T, (y_hat - y_true))) # Calculate Gradient Descent for the weights\n",
    "            db = (1 / len(X) * np.sum(y_hat - y_true)) # Calculate Gradient Descent for the bias\n",
    "            return dw, db\n",
    "        else :\n",
    "            dw = (1 / len(X) * np.dot(X.T, (y_hat - y_true)))\n",
    "            return dw\n",
    "        \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / ( 1 + np.exp(-z))\n",
    "        \n",
    "    def __logloss(self, y_true, y_pred, epsilon):\n",
    "        y_pred = np.clip(y_pred, a_min = epsilon, a_max = 1 - epsilon)\n",
    "        notation1 = y_true * np.log(y_pred + epsilon)\n",
    "        notation2 = ( 1 - y_true) * np.log(1 - y_pred + epsilon)\n",
    "        notation = notation1 + notation2\n",
    "        return - np.mean(notation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression :\n",
    "    def __init__(self, \n",
    "                 steps : int = 300, \n",
    "                 lr : float = 0.01,\n",
    "                 use_bias : bool = True,\n",
    "                 init : str = \"normal\"):\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "        self.use_bias = use_bias\n",
    "        self.init = init.lower()\n",
    "        \n",
    "    def fit(self, X, y, batch_size = 32):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        assert batch_size <= len(X), \"Batch size can't be bigger than size of the data.\"\n",
    "        assert len(X) == len(y), f\"Feature size {len(X)} has different size with label size len(y)\"\n",
    "        y_ohe = self.__OneHot(y)\n",
    "        losses = []\n",
    "        N, m = X.shape\n",
    "        if self.init == \"normal\":\n",
    "            w = np.random.normal(0, 0.05, size=(m, y_ohe.shape[1]))\n",
    "            b = np.random.normal(0, 0.05, size=(y_ohe.shape[1]))\n",
    "        elif self.init == \"uniform\":\n",
    "            w = np.random.uniform(low=-0.05, high=0.05, size=(m, y_ohe.shape[1]))\n",
    "            b = np.random.uniform(low=-0.05, high=0.05, size=(y_ohe.shape[1]))\n",
    "        else :\n",
    "            raise ValueError(\"Weights initializer is not valid.. Use normal or uniform.\")\n",
    "        \n",
    "        for _ in range(self.steps):\n",
    "            perm = np.random.permutation(N)\n",
    "            X_perm, y_perm, y_perm_ohe = X[perm], y[perm], y_ohe[perm]\n",
    "            for batch in range(0, N, batch_size):\n",
    "                X_batch, y_batch, y_ohe_batch = X_perm[batch:batch+batch_size], y_perm[batch:batch+batch_size], y_perm_ohe[batch:batch+batch_size]\n",
    "                if self.use_bias :\n",
    "                    y_prob = self.__softmax(np.dot(X_batch, w)) + b\n",
    "                    dw, db = self.__gradientDescent(X_batch, y_ohe_batch, y_prob)\n",
    "                    w = w - self.lr * dw\n",
    "                    b = b - self.lr * db\n",
    "                    loss = self.__categoryLogLoss(y_batch, y_prob)\n",
    "                    losses.append(loss)\n",
    "                     \n",
    "                else :\n",
    "                    z = self.__softmax(np.dot(X_batch, w))\n",
    "                    dw = self.__gradientDescent(X_batch, y_ohe_batch, y_prob)\n",
    "                    w -= self.lr * dw\n",
    "                    loss = SoftmaxRegression.categoryLogLoss(y_batch, y_prob)\n",
    "                    losses.append(loss)\n",
    "\n",
    "        self.w, self.b = w, b\n",
    "        self.loss_hist = np.array(losses)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        assert X.shape[1] == self.m, f\"{X.shape[1]} has not the same shape as fit !\"\n",
    "        if self.use_bias : \n",
    "            z = self.__softmax(np.dot(X, self.w))\n",
    "        else :\n",
    "            z = self.__softmax(np.dot(X, self.w))\n",
    "        return np.argmax(z, axis=-1)\n",
    "    \n",
    "    def __gradientDescent(self, X, y_true, y_hat):\n",
    "        if self.use_bias : \n",
    "            dw = (1 / len(X)) * np.dot(X.T, (y_hat - y_true))\n",
    "            db =  (1 / len(X)) * np.sum(y_hat - y_true)\n",
    "            return dw, db\n",
    "        else : \n",
    "            dw = (1 / len(X)) * np.dot(X.T, (y_hat - y_true))\n",
    "            return dw\n",
    "        \n",
    "    def __softmax(self, z):\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / exp_z.sum()\n",
    "    \n",
    "    def __OneHot(self, y):\n",
    "        y_ohe = np.zeros((len(y), len(np.unique(y))))\n",
    "        y_ohe[np.arange(len(y)), y] = 1\n",
    "        return y_ohe\n",
    "    \n",
    "    def __categoryLogLoss(self, y_true, y_pred):\n",
    "        return - np.mean(np.log(y_pred[np.arange(len(y_true)), y_true]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afdb29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(sc_train, y_train)\n",
    "\n",
    "sr = SoftmaxRegression()\n",
    "sr.fit(sc_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr.loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327690c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
