{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ac6990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.preprocess import Standardize\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f44deb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def L1Norm(X):\n",
    "#     return X / np.sum(np.abs(X), axis=0)\n",
    "\n",
    "# def L2Norm(X):\n",
    "#     return X / np.sqrt(np.sum(X ** 2, axis=0))\n",
    "\n",
    "# def LInf(X):\n",
    "#     return X / np.max(np.abs(np.sum(X, axis=0)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e2a5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1, l2, l_inf = L1Norm(y), L2Norm(y), LInf(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f774946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(np.arange(len(l1)), l1, label='L1', color='r', edgecolor='k')\n",
    "# plt.scatter(np.arange(len(l2)), l2, label='L2', color='blue', edgecolor='k')\n",
    "# plt.scatter(np.arange(len(l_inf)), l_inf, label='L-Inf', color='green', edgecolor='k')\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc394def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 30) (483,)\n",
      "(86, 30) (86,)\n"
     ]
    }
   ],
   "source": [
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=2022)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c4cbb",
   "metadata": {},
   "source": [
    "# Scaling / Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "020980a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = Standardize()\n",
    "sc.calc(X_train)\n",
    "\n",
    "sc_train = sc.scale(X_train)\n",
    "sc_test = sc.scale(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf3c7e",
   "metadata": {},
   "source": [
    "# Logistic Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce7f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression :\n",
    "    def __init__(self, \n",
    "                 steps : int = 30, \n",
    "                 epsilon : float = 1e-6,\n",
    "                 lr : float = 0.01,\n",
    "                 threshold : float = 0.5,\n",
    "                 use_bias : bool = True,\n",
    "                 init : str = \"normal\"):\n",
    "        self.use_bias = use_bias\n",
    "        self.steps = steps\n",
    "        self.epsilon = epsilon\n",
    "        self.init = init.lower()\n",
    "        self.lr = lr\n",
    "        assert threshold < 1.0 and threshold > 0.0, f\"Threshold has to be between 0 and 1 !\"\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        N, m = X.shape\n",
    "        if self.init == \"normal\":\n",
    "            w = np.random.normal(loc=0., scale=0.05, size=m)\n",
    "            b = np.random.normal(loc=0., scale=0.05, size=1)\n",
    "        elif self.init == \"uniform\":\n",
    "            w = np.random.uniform(low=-0.05, high=0.05, size=m)\n",
    "            b = np.random.uniform(low=-0.05, high=0.05, size=1)\n",
    "        else :\n",
    "            raise ValueError(\"Weights initializer is not valid. Use uniform or normal.\")\n",
    "        assert X.shape[0] == y.shape[0], f\"Feature size {X.shape[0]} has not the same as label size {y.shape[0]}\"            \n",
    "        losses = []\n",
    "        \n",
    "        for _ in range(self.steps):\n",
    "            if self.use_bias :\n",
    "                y_prob = LogisticRegression.sigmoid(np.dot(X, w)) + b\n",
    "                dw = (1 / m) * np.dot(X.T, (y_prob - y))\n",
    "                db = (1 / m) * np.sum(y_prob - y)\n",
    "                w = w - self.lr * dw\n",
    "                b = b - self.lr * db\n",
    "                loss = LogisticRegression.logloss(y, y_prob, self.epsilon)\n",
    "                if _ % 10 == 0:\n",
    "                    print(f\"Epochs : {_ + 1} => Loss : {loss}\")\n",
    "                losses.append(loss)\n",
    "            else : \n",
    "                y_prob = LogisticRegression.sigmoid(np.dot(X, w)) # feedforward\n",
    "                dw = (1 / m) * np.dot(X.T, (y_prob - y))\n",
    "                w = w - self.lr * dw\n",
    "                loss = LogisticRegression.logloss(y, y_prob, self.epsilon)\n",
    "                if _ % 10 == 0 :\n",
    "                    print(f\"Epochs : {_ + 1} => Loss : {loss}\")\n",
    "                losses.append(loss)\n",
    "        self.w, self.b = w, b\n",
    "        \n",
    "    def predict(self, X):\n",
    "        assert X.shape[1] == len(self.w), \"Different shape with fitted data !\"\n",
    "        if self.use_bias :\n",
    "            z = LogisticRegression.sigmoid(np.dot(X, self.w)) + self.b\n",
    "        else : \n",
    "            z = LogisticRegression.sigmoid(np.dot(X, self.w))\n",
    "        return np.array([1 if i > self.threshold else 0 for i in z])\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / ( 1 + np.exp(-z))\n",
    "        \n",
    "    @staticmethod\n",
    "    def logloss(y_true, y_pred, epsilon):\n",
    "        y_pred = np.clip(y_pred, a_min = epsilon, a_max = 1 - epsilon)\n",
    "        notation1 = y_true * np.log(y_pred + epsilon)\n",
    "        notation2 = ( 1 - y_true) * np.log(1 - y_pred + epsilon)\n",
    "        notation = notation1 + notation2\n",
    "        return - np.mean(notation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed9cf90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6274165202108963"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)[1][1] / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c5454b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 1 => Loss : 0.7564470992831839\n",
      "Epochs : 11 => Loss : 0.19200822811050708\n",
      "Epochs : 21 => Loss : 0.1533364584496294\n",
      "Epochs : 31 => Loss : 0.134967811939539\n",
      "Epochs : 41 => Loss : 0.12367017970009665\n",
      "Epochs : 51 => Loss : 0.11585643762737763\n",
      "Epochs : 61 => Loss : 0.11005974533758409\n",
      "Epochs : 71 => Loss : 0.10552666310895942\n",
      "Epochs : 81 => Loss : 0.10185252524696412\n",
      "Epochs : 91 => Loss : 0.09879389505142808\n",
      "Epochs : 101 => Loss : 0.09620735728126532\n",
      "Epochs : 111 => Loss : 0.09397687683600844\n",
      "Epochs : 121 => Loss : 0.09202007785254486\n",
      "Epochs : 131 => Loss : 0.0902857970027842\n",
      "Epochs : 141 => Loss : 0.08873331757445399\n",
      "Epochs : 151 => Loss : 0.08733212851743401\n",
      "Epochs : 161 => Loss : 0.08605747198818418\n",
      "Epochs : 171 => Loss : 0.08488995687443582\n",
      "Epochs : 181 => Loss : 0.08381537504709535\n",
      "Epochs : 191 => Loss : 0.08282255980372068\n",
      "Epochs : 201 => Loss : 0.0818990984326484\n",
      "Epochs : 211 => Loss : 0.08103641411605031\n",
      "Epochs : 221 => Loss : 0.08022877191575165\n",
      "Epochs : 231 => Loss : 0.07947031560748313\n",
      "Epochs : 241 => Loss : 0.0787546333967976\n",
      "Epochs : 251 => Loss : 0.07807745547122569\n",
      "Epochs : 261 => Loss : 0.07743477425831971\n",
      "Epochs : 271 => Loss : 0.07682338593177888\n",
      "Epochs : 281 => Loss : 0.07624031465068709\n",
      "Epochs : 291 => Loss : 0.0756835306706656\n",
      "Epochs : 301 => Loss : 0.07515048825960527\n",
      "Epochs : 311 => Loss : 0.07463918877981698\n",
      "Epochs : 321 => Loss : 0.07414822506015782\n",
      "Epochs : 331 => Loss : 0.07367589522655685\n",
      "Epochs : 341 => Loss : 0.07322088552574069\n",
      "Epochs : 351 => Loss : 0.07278215767034671\n",
      "Epochs : 361 => Loss : 0.07235856197305988\n",
      "Epochs : 371 => Loss : 0.07194913025231667\n",
      "Epochs : 381 => Loss : 0.07155301538439744\n",
      "Epochs : 391 => Loss : 0.071169448934808\n",
      "Epochs : 401 => Loss : 0.07079773153207954\n",
      "Epochs : 411 => Loss : 0.07043722460013728\n",
      "Epochs : 421 => Loss : 0.07008739547024599\n",
      "Epochs : 431 => Loss : 0.06974776252696245\n",
      "Epochs : 441 => Loss : 0.06941770766552648\n",
      "Epochs : 451 => Loss : 0.06909677464752147\n",
      "Epochs : 461 => Loss : 0.06878454075401264\n",
      "Epochs : 471 => Loss : 0.06848061332283857\n",
      "Epochs : 481 => Loss : 0.06818462672750653\n",
      "Epochs : 491 => Loss : 0.0678962397348962\n",
      "Epochs : 501 => Loss : 0.06761513318841363\n",
      "Epochs : 511 => Loss : 0.06734100797119817\n",
      "Epochs : 521 => Loss : 0.06707358321072289\n",
      "Epochs : 531 => Loss : 0.06681259469183053\n",
      "Epochs : 541 => Loss : 0.06655779345007395\n",
      "Epochs : 551 => Loss : 0.06630894452133497\n",
      "Epochs : 561 => Loss : 0.06606582582716673\n",
      "Epochs : 571 => Loss : 0.06582822717826521\n",
      "Epochs : 581 => Loss : 0.06559594938098094\n",
      "Epochs : 591 => Loss : 0.06536880343392092\n",
      "Epochs : 601 => Loss : 0.0651466098035074\n",
      "Epochs : 611 => Loss : 0.0649291977689056\n",
      "Epochs : 621 => Loss : 0.06471640482805348\n",
      "Epochs : 631 => Loss : 0.06450808561798961\n",
      "Epochs : 641 => Loss : 0.06430413129023711\n",
      "Epochs : 651 => Loss : 0.06410435314219902\n",
      "Epochs : 661 => Loss : 0.06390861653004004\n",
      "Epochs : 671 => Loss : 0.06371679276161048\n",
      "Epochs : 681 => Loss : 0.06352875874779497\n",
      "Epochs : 691 => Loss : 0.06334439668097219\n",
      "Epochs : 701 => Loss : 0.06316359373786407\n",
      "Epochs : 711 => Loss : 0.0629862418043856\n",
      "Epochs : 721 => Loss : 0.06281223722038927\n",
      "Epochs : 731 => Loss : 0.06264148054244463\n",
      "Epochs : 741 => Loss : 0.062473876323006446\n",
      "Epochs : 751 => Loss : 0.06230933290451058\n",
      "Epochs : 761 => Loss : 0.06214776222709619\n",
      "Epochs : 771 => Loss : 0.0619890796487963\n",
      "Epochs : 781 => Loss : 0.06183320377715733\n",
      "Epochs : 791 => Loss : 0.06168005631135986\n",
      "Epochs : 801 => Loss : 0.06152956189400357\n",
      "Epochs : 811 => Loss : 0.06138164797180545\n",
      "Epochs : 821 => Loss : 0.06123625900322396\n",
      "Epochs : 831 => Loss : 0.06109335123429095\n",
      "Epochs : 841 => Loss : 0.06095282164106037\n",
      "Epochs : 851 => Loss : 0.06081460763533402\n",
      "Epochs : 861 => Loss : 0.0606786488497165\n",
      "Epochs : 871 => Loss : 0.060544887039462814\n",
      "Epochs : 881 => Loss : 0.06041327292574012\n",
      "Epochs : 891 => Loss : 0.060283785126177276\n",
      "Epochs : 901 => Loss : 0.060156331050169765\n",
      "Epochs : 911 => Loss : 0.06003086006201427\n",
      "Epochs : 921 => Loss : 0.059907323230993244\n",
      "Epochs : 931 => Loss : 0.059785673260210184\n",
      "Epochs : 941 => Loss : 0.05966587082691345\n",
      "Epochs : 951 => Loss : 0.05954789497090822\n",
      "Epochs : 961 => Loss : 0.05943167320451632\n",
      "Epochs : 971 => Loss : 0.059317164109730376\n",
      "Epochs : 981 => Loss : 0.05920432759670438\n",
      "Epochs : 991 => Loss : 0.059093124850978215\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(threshold=0.6274, steps=1000)\n",
    "log_reg.fit(sc_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77b3fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  0.9813664596273292\n",
      "Test Accuracy :  0.9651162790697675\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy : \", np.sum(log_reg.predict(sc_train) == y_train) / len(y_train))\n",
    "print(\"Test Accuracy : \", np.sum(log_reg.predict(sc_test) == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700263e",
   "metadata": {},
   "source": [
    "# Softmax Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91517941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 4) (127,)\n",
      "(23, 4) (23,)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=2022)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5958fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHot(y):\n",
    "    y_ohe = np.zeros((len(y), y.max() + 1))\n",
    "    y_ohe[np.arange(len(y)), y] = 1\n",
    "    return y_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12e973d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = Standardize()\n",
    "sc.calc(X_train)\n",
    "\n",
    "sc_train = sc.scale(X_train)\n",
    "sc_test = sc.scale(X_test)\n",
    "\n",
    "train_ohe = OneHot(y_train)\n",
    "test_ohe = OneHot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "304fb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression :\n",
    "    def __init__(self, \n",
    "                 steps : int = 10, \n",
    "                 lr : float = 0.01,\n",
    "                 use_bias : bool = True,\n",
    "                 epsilon : float = 1e-7,\n",
    "                 init : str = \"normal\"):\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "        self.use_bias = use_bias\n",
    "        self.init = init.lower()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        assert len(X) == len(y), f\"Feature size {len(X)} has different size with label size len(y)\"\n",
    "        y_ohe = SoftmaxRegression.OneHot(y)\n",
    "        losses = []\n",
    "        N, m = X.shape\n",
    "        if self.init == \"normal\":\n",
    "            w = np.random.normal(0, 0.05, size=(m, y_ohe.shape[1]))\n",
    "            b = np.random.normal(0, 0.05, size=(y_ohe.shape[1]))\n",
    "        elif self.init == \"uniform\":\n",
    "            w = np.random.uniform(low=-0.0, high=1., size=(m, y_ohe.shape[1]))\n",
    "            b = np.random.uniform(low=0., high=1., size=(y_ohe.shape[1]))\n",
    "        else :\n",
    "            raise ValueError(\"Weights initializer is not valid.. Use normal or uniform.\")\n",
    "        \n",
    "        for _ in range(self.steps):\n",
    "            if self.use_bias :\n",
    "                z = SoftmaxRegression.softmax(np.dot(X, w)) + b\n",
    "                dw = (1 / m) * np.dot(X.T, (z - y_ohe))\n",
    "                db = (1 / m) * np.sum(z - y_ohe)\n",
    "                w -= self.lr * dw\n",
    "                b -= self.lr * db\n",
    "                loss = SoftmaxRegression.categoryLogLoss(y, z)\n",
    "                if _ % 10 == 0:\n",
    "                    print(f\"Epochs : {_ + 1} => Loss : {loss}\")\n",
    "            else :\n",
    "                z = SoftmaxRegression.softmax(np.dot(X, w))\n",
    "                dw = (1 / m) * np.dot(X.T, (z - y_ohe))\n",
    "                w -= self.lr * dw\n",
    "                loss = SoftmaxRegression.categoryLogLoss(y, z)\n",
    "                if _ % 10 == 0:\n",
    "                    print(f\"Epochs: {_ + 1} => Loss : {loss}\")\n",
    "            losses.append(loss)\n",
    "                \n",
    "        self.w, self.b = w, b\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.use_bias : \n",
    "            z = SoftmaxRegression.softmax(np.dot(X, self.w))\n",
    "        else :\n",
    "            z = SoftmaxRegression.softmax(np.dot(X, self.w))\n",
    "        return np.argmax(z, axis=-1)\n",
    "        \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / exp_z.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def OneHot(y):\n",
    "        y_ohe = np.zeros((len(y), len(np.unique(y))))\n",
    "        y_ohe[np.arange(len(y)), y] = 1\n",
    "        return y_ohe\n",
    "    \n",
    "    @staticmethod\n",
    "    def categoryLogLoss(y_true, y_pred):\n",
    "        return - np.mean(np.log(y_pred[np.arange(len(y_true)), y_true]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81f34232",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 1 => Loss : nan\n",
      "Epochs : 11 => Loss : 1.0866982240215735\n",
      "Epochs : 21 => Loss : 1.0873132188406807\n",
      "Epochs : 31 => Loss : 1.0893569272598695\n",
      "Epochs : 41 => Loss : 1.0913646766519425\n",
      "Epochs : 51 => Loss : 1.0928629064729656\n",
      "Epochs : 61 => Loss : 1.0938653855174323\n",
      "Epochs : 71 => Loss : 1.0945201996123566\n",
      "Epochs : 81 => Loss : 1.0949534357945523\n",
      "Epochs : 91 => Loss : 1.0952473552874038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-b94bf8667166>:69: RuntimeWarning: invalid value encountered in log\n",
      "  return - np.mean(np.log(y_pred[np.arange(len(y_true)), y_true]))\n"
     ]
    }
   ],
   "source": [
    "sr = SoftmaxRegression(use_bias=True, steps =100)\n",
    "sr.fit(sc_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d3ff2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  0.7716535433070866\n",
      "Test Accuracy :  0.8695652173913043\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy : \", np.sum(sr.predict(sc_train) == y_train) / len(y_train))\n",
    "print(\"Test Accuracy : \", np.sum(sr.predict(sc_test) == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49f60f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression :\n",
    "    def __init__(self, \n",
    "                 steps : int = 30, \n",
    "                 epsilon : float = 1e-6,\n",
    "                 lr : float = 0.01,\n",
    "                 threshold : float = 0.5,\n",
    "                 use_bias : bool = True,\n",
    "                 init : str = \"normal\"):\n",
    "        self.use_bias = use_bias\n",
    "        self.steps = steps\n",
    "        self.epsilon = epsilon\n",
    "        self.init = init.lower()\n",
    "        self.lr = lr\n",
    "        assert threshold < 1.0 and threshold > 0.0, f\"Threshold has to be between 0 and 1 !\"\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        N, m = X.shape\n",
    "        if self.init == \"normal\":\n",
    "            w = np.random.normal(loc=0., scale=0.05, size=m)\n",
    "            b = np.random.normal(loc=0., scale=0.05, size=1)\n",
    "        elif self.init == \"uniform\":\n",
    "            w = np.random.uniform(low=-0.05, high=0.05, size=m)\n",
    "            b = np.random.uniform(low=-0.05, high=0.05, size=1)\n",
    "        else :\n",
    "            raise ValueError(\"Weights initializer is not valid. Use uniform or normal.\")\n",
    "        assert X.shape[0] == y.shape[0], f\"Feature size {X.shape[0]} has not the same as label size {y.shape[0]}\"            \n",
    "        losses = []\n",
    "        \n",
    "        for _ in range(self.steps):\n",
    "            if self.use_bias :\n",
    "                y_prob = LogisticRegression.sigmoid(np.dot(X, w)) + b\n",
    "                dw = (1 / m) * np.dot(X.T, (y_prob - y))\n",
    "                db = (1 / m) * np.sum(y_prob - y)\n",
    "                w = w - self.lr * dw\n",
    "                b = b - self.lr * db\n",
    "                loss = LogisticRegression.logloss(y, y_prob, self.epsilon)\n",
    "                if _ % 10 == 0:\n",
    "                    print(f\"Epochs : {_ + 1} => Loss : {loss}\")\n",
    "                losses.append(loss)\n",
    "            else : \n",
    "                y_prob = LogisticRegression.sigmoid(np.dot(X, w)) # feedforward\n",
    "                dw = (1 / m) * np.dot(X.T, (y_prob - y))\n",
    "                w = w - self.lr * dw\n",
    "                loss = LogisticRegression.logloss(y, y_prob, self.epsilon)\n",
    "                if _ % 10 == 0 :\n",
    "                    print(f\"Epochs : {_ + 1} => Loss : {loss}\")\n",
    "                losses.append(loss)\n",
    "        self.w, self.b = w, b\n",
    "        \n",
    "    def predict(self, X):\n",
    "        assert X.shape[1] == len(self.w), \"Different shape with fitted data !\"\n",
    "        if self.use_bias :\n",
    "            z = LogisticRegression.sigmoid(np.dot(X, self.w)) + self.b\n",
    "        else : \n",
    "            z = LogisticRegression.sigmoid(np.dot(X, self.w))\n",
    "        return np.array([1 if i > self.threshold else 0 for i in z])\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / ( 1 + np.exp(-z))\n",
    "        \n",
    "    @staticmethod\n",
    "    def logloss(y_true, y_pred, epsilon):\n",
    "        y_pred = np.clip(y_pred, a_min = epsilon, a_max = 1 - epsilon)\n",
    "        notation1 = y_true * np.log(y_pred + epsilon)\n",
    "        notation2 = ( 1 - y_true) * np.log(1 - y_pred + epsilon)\n",
    "        notation = notation1 + notation2\n",
    "        return - np.mean(notation)\n",
    "    \n",
    "class SoftmaxRegression :\n",
    "    def __init__(self, \n",
    "                 steps : int = 10, \n",
    "                 lr : float = 0.01,\n",
    "                 use_bias : bool = True,\n",
    "                 epsilon : float = 1e-7,\n",
    "                 init : str = \"normal\"):\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "        self.use_bias = use_bias\n",
    "        self.init = init.lower()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        assert len(X) == len(y), f\"Feature size {len(X)} has different size with label size len(y)\"\n",
    "        y_ohe = SoftmaxRegression.OneHot(y)\n",
    "        losses = []\n",
    "        N, m = X.shape\n",
    "        if self.init == \"normal\":\n",
    "            w = np.random.normal(0, 0.05, size=(m, y_ohe.shape[1]))\n",
    "            b = np.random.normal(0, 0.05, size=(y_ohe.shape[1]))\n",
    "        elif self.init == \"uniform\":\n",
    "            w = np.random.uniform(low=-0.0, high=1., size=(m, y_ohe.shape[1]))\n",
    "            b = np.random.uniform(low=0., high=1., size=(y_ohe.shape[1]))\n",
    "        else :\n",
    "            raise ValueError(\"Weights initializer is not valid.. Use normal or uniform.\")\n",
    "        \n",
    "        for _ in range(self.steps):\n",
    "            if self.use_bias :\n",
    "                z = SoftmaxRegression.softmax(np.dot(X, w)) + b\n",
    "                dw = (1 / m) * np.dot(X.T, (z - y_ohe))\n",
    "                db = (1 / m) * np.sum(z - y_ohe)\n",
    "                w -= self.lr * dw\n",
    "                b -= self.lr * db\n",
    "                loss = SoftmaxRegression.categoryLogLoss(y, z)\n",
    "                if _ % 10 == 0:\n",
    "                    print(f\"Epochs : {_ + 1} => Loss : {loss}\")\n",
    "            else :\n",
    "                z = SoftmaxRegression.softmax(np.dot(X, w))\n",
    "                dw = (1 / m) * np.dot(X.T, (z - y_ohe))\n",
    "                w -= self.lr * dw\n",
    "                loss = SoftmaxRegression.categoryLogLoss(y, z)\n",
    "                if _ % 10 == 0:\n",
    "                    print(f\"Epochs: {_ + 1} => Loss : {loss}\")\n",
    "            losses.append(loss)\n",
    "                \n",
    "        self.w, self.b = w, b\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.use_bias : \n",
    "            z = SoftmaxRegression.softmax(np.dot(X, self.w))\n",
    "        else :\n",
    "            z = SoftmaxRegression.softmax(np.dot(X, self.w))\n",
    "        return np.argmax(z, axis=-1)\n",
    "        \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / exp_z.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def OneHot(y):\n",
    "        y_ohe = np.zeros((len(y), len(np.unique(y))))\n",
    "        y_ohe[np.arange(len(y)), y] = 1\n",
    "        return y_ohe\n",
    "    \n",
    "    @staticmethod\n",
    "    def categoryLogLoss(y_true, y_pred):\n",
    "        return - np.mean(np.log(y_pred[np.arange(len(y_true)), y_true]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
